# Global Settings
nameOverride: ""
fullnameOverride: ""

namespace:
  create: true
  name: "vllm-cluster"

# Image settings
image:
  repository: vllm/vllm-openai:latest
  workerRepository: vllm/vllm-openai:latest
  pullPolicy: IfNotPresent

# Head Node Configuration
headNode:
  name: "vllm-head"
  replicas: 1
  resources:
    limits:
      nvidia.com/gpu: 1
    requests:
      nvidia.com/gpu: 1
  nodeSelector:
    role: master
  ports:
    ray: 6379
    vllm: 8000
  command: 
    - "/bin/sh"
    - "-c"
    - "ray start --head --port=6379 --block && tail -f /dev/null"

# Worker Node Configuration
workerNodes:
  enabled: true
  # Node selector for all worker nodes
  nodeSelector:
    role: worker
  tolerations:
    - key: "role"
      operator: "Equal"
      value: "worker"
      effect: "NoSchedule"
  resources:
    limits:
      nvidia.com/gpu: 1
    requests:
      nvidia.com/gpu: 1
  ports:
    vllm: 8000
  command:
    - "/bin/sh"
    - "-c"
    - "ray start --address=vllm-head-service:6379 --block"


# Volume Mounts Configuration
volumes:
  - name: huggingface-cache
    hostPath:
      path: /root/.cache/huggingface
  - name: benchmarks
    hostPath:
      path: /home/ubuntu/models

volumeMounts:
  - name: huggingface-cache
    mountPath: /root/.cache/huggingface
  - name: model-cache
    mountPath: /vllm-workspace/examples

# Service Configuration
service:
  head:
    type: ClusterIP
    port: 6379
  vllm:
    type: NodePort
    port: 8000
    nodePort: 30080

# Additional Configuration
additionalEnvVars: {}
additionalLabels: {}
additionalAnnotations: {}